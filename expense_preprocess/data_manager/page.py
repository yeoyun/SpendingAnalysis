# expense_preprocess/data_manager/page.py
from __future__ import annotations

import streamlit as st
import pandas as pd

from expense_preprocess.data_gen.raw_like import generate_test_raw_df
from expense_preprocess.data_gen.ui_test_data import render_test_data_generator

from .state import (
    init_data_manager_state,
    add_uploaded_file,
    get_raw_files,
    get_upload_log,
    get_clean_files,
    save_clean_df,
    set_active_df,
    get_active_df,
    get_active_source,
    get_timeline_max_date,
    delete_file,
    clear_active,
    clear_all,
    patch_clean_meta,
    SOURCE_COL,
)
from .io import load_df_from_bytes, ensure_date_col

from expense_preprocess.preprocess import run_preprocess


def _fmt_iso(iso: str | None) -> str:
    return iso if iso else "-"


def _date_only_series(s: pd.Series) -> pd.Series:
    d = pd.to_datetime(s, errors="coerce")
    return d.dt.date.astype(str)


def _incremental_append_by_day(
    active_df: pd.DataFrame | None,
    new_df: pd.DataFrame,
    *,
    source_name: str,
) -> tuple[pd.DataFrame, dict]:
    """
    âœ… ìš”êµ¬ì‚¬í•­ 3:
    - ê¸°ì¡´ì— ì¡´ì¬í•˜ëŠ” 'ë‚ ì§œ(date)'ëŠ” ì‹ ê·œì—ì„œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ ì¶”ê°€
    - ì•/ë’¤ ê¸°ê°„ ëª¨ë‘ í—ˆìš©
    - ì´ë²ˆì— ì‹¤ì œ ì¶”ê°€ëœ í–‰ì— SOURCE_COL(=__source_file)ì„ ë°•ì•„ ë‘  (ìš”êµ¬ì‚¬í•­ 1 ì‚­ì œ ê°€ëŠ¥)

    return:
      merged_df, merge_meta(dict)
    """
    new_df = ensure_date_col(new_df).copy()
    new_df[SOURCE_COL] = source_name
    new_df["__date_only"] = _date_only_series(new_df["date"])

    # active ì—†ìŒ: ì „ë¶€ ì¶”ê°€
    if active_df is None or active_df.empty:
        merged = new_df.sort_values("date").reset_index(drop=True)

        meta = {
            "added_rows": int(len(new_df)),
            "dropped_duplicate_days": 0,
            "added_min_date": (pd.to_datetime(new_df["date"]).min().isoformat() if len(new_df) else None),
            "added_max_date": (pd.to_datetime(new_df["date"]).max().isoformat() if len(new_df) else None),
        }
        return merged, meta

    active_df = ensure_date_col(active_df).copy()
    if "date" not in active_df.columns:
        merged = pd.concat([active_df, new_df], ignore_index=True)
        meta = {"added_rows": int(len(new_df)), "dropped_duplicate_days": 0, "added_min_date": None, "added_max_date": None}
        return merged, meta

    active_df["__date_only"] = _date_only_series(active_df["date"])
    existing_days = set(active_df["__date_only"].dropna().unique().tolist())

    # âœ… ë‚ ì§œ ì¤‘ë³µë§Œ ì œì™¸
    dup_mask = new_df["__date_only"].isin(existing_days)
    dropped = int(dup_mask.sum())

    add_part = new_df.loc[~dup_mask].copy()

    merged = pd.concat([active_df, add_part], ignore_index=True)
    merged = merged.sort_values("date").reset_index(drop=True)

    # ë‚´ë¶€ ì»¬ëŸ¼ ì •ë¦¬(ì›í•˜ì‹œë©´ ë‚¨ê²¨ë„ ë¨)
    merged = merged.drop(columns=["__date_only"], errors="ignore")

    added_min = None
    added_max = None
    if len(add_part) > 0:
        added_min = pd.to_datetime(add_part["date"], errors="coerce").dropna().min()
        added_max = pd.to_datetime(add_part["date"], errors="coerce").dropna().max()

    meta = {
        "added_rows": int(len(add_part)),
        "dropped_duplicate_days": dropped,
        "added_min_date": (added_min.isoformat() if added_min is not None and pd.notna(added_min) else None),
        "added_max_date": (added_max.isoformat() if added_max is not None and pd.notna(added_max) else None),
    }
    return merged, meta


def render_data_manage_page() -> None:
    init_data_manager_state()

    st.header("ğŸ—‚ï¸ ë°ì´í„° ê´€ë¦¬")

    raw_files = get_raw_files()
    upload_log = get_upload_log()
    clean_files = get_clean_files()

    names = sorted(set(upload_log.keys()) | set(clean_files.keys()) | set(raw_files.keys()))

    if not names:
        st.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    else:
        st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„° ì¶”ê°€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # =========================
    # í˜„ì¬ íƒ€ì„ë¼ì¸(í™œì„± ë°ì´í„° ê¸°ì¤€)
    # =========================
    active_src = get_active_source()
    df_active = get_active_df()

    c_active = clean_files.get(active_src, None) if active_src else None

    if df_active is None or df_active.empty:
        st.write("- í™œì„± ë°ì´í„° ì—†ìŒ")
    else:
        min_ts = None
        max_ts = None
        if "date" in df_active.columns:
            s = pd.to_datetime(df_active["date"], errors="coerce").dropna()
            if not s.empty:
                min_ts = s.min()
                max_ts = s.max()

        min_str = min_ts.strftime("%Y-%m-%d") if min_ts is not None else "-"
        max_str = max_ts.strftime("%Y-%m-%d") if max_ts is not None else "-"

        range_min = c_active.get("min_date") if c_active else None
        range_max = c_active.get("max_date") if c_active else None

        st.write(f"##### {(range_min or min_str)} ~ {(range_max or max_str)}")
        st.write(f"- í™œì„± ì†ŒìŠ¤ (ë§ˆì§€ë§‰ ì²˜ë¦¬ ê¸°ì¤€): **{active_src or '-'}**")
        st.write(f"- í¬ê¸°: **{df_active.shape[0]:,} rows Ã— {df_active.shape[1]:,} cols**")

    # =========================
    # ì—…ë¡œë“œ ëª©ë¡ + ë¡œê·¸/ì •ì œ ìƒíƒœ
    # =========================
    st.markdown("### ğŸ“ ì—…ë¡œë“œ ëª©ë¡")

    default_idx = 0
    if active_src in names:
        default_idx = names.index(active_src)

    st.markdown("íŒŒì¼ ì„ íƒ")
    colA, colB = st.columns([1.2, 0.8])
    with colA:
        selected = st.selectbox(
            "íŒŒì¼ ì„ íƒ",
            names,
            index=default_idx,
            key="dm_selected",
            label_visibility="collapsed",
        )
    with colB:
        if st.button("ğŸ—‘ï¸ ì„ íƒ íŒŒì¼ ì‚­ì œ", use_container_width=True):
            delete_file(selected)
            st.rerun()

    u = upload_log.get(selected, {})
    c = clean_files.get(selected, None)

    st.write(f"- ì—…ë¡œë“œ ì‹œê°: **{_fmt_iso(u.get('uploaded_at'))}** / size: {u.get('size_bytes', 0):,} bytes")
    if c:
        st.write(f"- ì •ì œ rows: **{c.get('rows', 0):,}** / range: {c.get('min_date') or '-'} ~ {c.get('max_date') or '-'}")
        if c.get("added_rows") is not None:
            st.write(
                f"- âœ… í™œì„± ë°˜ì˜(ì¦ë¶„) ê²°ê³¼: **ì¶”ê°€ {c.get('added_rows', 0):,}í–‰**, "
                f"ì¤‘ë³µë‚ ì§œ ì œì™¸ {c.get('dropped_duplicate_days', 0):,}í–‰ "
                f"/ ì¶”ê°€ê¸°ê°„: {c.get('added_min_date') or '-'} ~ {c.get('added_max_date') or '-'}"
            )
    else:
        st.write("- ì „ì²˜ë¦¬ ìƒíƒœ: ë¯¸ìƒì„±")

    if selected not in raw_files:
        st.warning(
            "ì´ íŒŒì¼ì€ ì—…ë¡œë“œ/ì •ì œ ë©”íƒ€(ë””ìŠ¤í¬ ìŠ¤ëƒ…ìƒ·)ëŠ” ë‚¨ì•„ìˆì§€ë§Œ, "
            "ì›ë³¸(raw bytes)ì€ í˜„ì¬ ì„¸ì…˜ì— ë‚¨ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
            "ì›ë³¸ì„ ë‹¤ì‹œ ì½ê³  ì‹¶ì€ ê²½ìš°, ì¬ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        )

    st.divider()

    # =========================
    # âœ… ì„ íƒ íŒŒì¼ ì •ì œ + í™œì„± ë°ì´í„° ë°˜ì˜ ë²„íŠ¼
    # =========================
    st.markdown("### ğŸ§¼ ì„ íƒ íŒŒì¼ ì •ì œ & í™œì„± ë°˜ì˜")

    can_process = selected in raw_files
    if not can_process:
        st.info("ì„ íƒ íŒŒì¼ì˜ ì›ë³¸ bytesê°€ ì„¸ì…˜ì— ì—†ì–´ ì •ì œë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¬ì—…ë¡œë“œ í›„ ì§„í–‰í•´ì£¼ì„¸ìš”.")
    else:
        if st.button("âœ¨ ì •ì œ ì‹¤í–‰ í›„ í™œì„± ë°ì´í„°ì— ë°˜ì˜", use_container_width=True, key="dm_run_preprocess_apply"):
            try:
                raw_bytes = raw_files[selected]
                df_raw = load_df_from_bytes(selected, raw_bytes)
                df_clean = run_preprocess(df_raw)

                # 1) clean file ì €ì¥(ë””ìŠ¤í¬) + ë©”íƒ€ ì €ì¥
                save_clean_df(selected, df_clean)

                # 2) í™œì„± ë°ì´í„°ì— 'ë‚ ì§œ ì¤‘ë³µë§Œ ì œì™¸' ì¦ë¶„ ë°˜ì˜
                df_active_now = get_active_df()
                merged, merge_meta = _incremental_append_by_day(
                    df_active_now,
                    df_clean,
                    source_name=selected,
                )
                set_active_df(merged, source_name=selected)

                # 3) clean_files ë©”íƒ€ì— â€œì¦ë¶„ ë°˜ì˜ ê²°ê³¼â€ ê¸°ë¡
                patch_clean_meta(selected, merge_meta)

                st.success(
                    f"ë°˜ì˜ ì™„ë£Œ! ì¶”ê°€ {merge_meta['added_rows']:,}í–‰ / "
                    f"ì¤‘ë³µë‚ ì§œ ì œì™¸ {merge_meta['dropped_duplicate_days']:,}í–‰"
                )
                st.rerun()

            except Exception as e:
                st.error("ì •ì œ/ë°˜ì˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                st.exception(e)

    st.divider()

    # =========================================================
    # âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± UI
    # =========================================================
    render_test_data_generator(
        generate_func=generate_test_raw_df,
        cache_key="dm_test_df_cache",
        expander_title="ê¸°ê°„ ì…ë ¥ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°",
        default_days=30,
        save_subdir="data/test_generated",
    )

    st.divider()

    # =========================
    # í˜„ì¬ í™œì„± ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    # =========================
    df_active = get_active_df()
    if df_active is None or df_active.empty:
        st.warning("í˜„ì¬ í™œì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    with st.expander("ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 50í–‰)"):
        st.dataframe(df_active.head(50), use_container_width=True)

    # âœ… ì „ì²´ ì‚­ì œ ë²„íŠ¼(í•˜ë‹¨ ë‹¨ì¼)
    if st.button("ğŸ§¨ ì „ì²´ ë°ì´í„° ì‚­ì œ", use_container_width=True, key="dm_clear_all_bottom"):
        clear_all()
        st.rerun()
        return

    # =========================
    # ë‚´ë³´ë‚´ê¸°: íŒŒì¼ëª… = ë°ì´í„° ê¸°ê°„(min~max)
    # =========================
    min_ts = None
    max_ts = None
    if "date" in df_active.columns:
        s = pd.to_datetime(df_active["date"], errors="coerce").dropna()
        if not s.empty:
            min_ts = s.min()
            max_ts = s.max()

    min_str = min_ts.strftime("%Y-%m-%d") if min_ts is not None else None
    max_str = max_ts.strftime("%Y-%m-%d") if max_ts is not None else None

    if min_str and max_str:
        export_name = f"active_{min_str}.csv" if min_str == max_str else f"active_{min_str}_{max_str}.csv"
    else:
        export_name = "active_data.csv"

    from pathlib import Path
    PROJECT_ROOT = Path(__file__).resolve().parents[2]
    save_dir = PROJECT_ROOT / "data" / "active"
    save_dir.mkdir(parents=True, exist_ok=True)

    save_path = save_dir / export_name

    try:
        df_active.to_csv(save_path, index=False, encoding="utf-8-sig")
        st.caption(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {save_path}")
    except Exception as e:
        st.warning("ì„œë²„ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ê¶Œí•œ/ê²½ë¡œ ë¬¸ì œ ê°€ëŠ¥)")
        st.exception(e)

    csv_bytes = df_active.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        label="â¬‡ï¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (.csv)",
        data=csv_bytes,
        file_name=export_name,
        mime="text/csv",
        use_container_width=True,
        key="dm_export_active_csv",
    )